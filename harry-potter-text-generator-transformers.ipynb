{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/moxxis/harry-potter-text-generator-transformers?scriptVersionId=107371041\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# IMPORTS\nimport re\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom torch.utils.data import Dataset, Subset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2022-10-06T15:51:28.742789Z","iopub.execute_input":"2022-10-06T15:51:28.743213Z","iopub.status.idle":"2022-10-06T15:51:28.754234Z","shell.execute_reply.started":"2022-10-06T15:51:28.743158Z","shell.execute_reply":"2022-10-06T15:51:28.752991Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#DIRECTORIES\n\nDATA_PATH = \"../input/harry-potter-lstm/Harry_Potter_all_books_preprocessed.txt\"\n#SAVED_MODEL_PATH = os.scandir('/kaggle/input/harry-potter-text-generator-transformers/weights').__next__().path\nSAVED_MODEL_PATH = os.scandir('../input/distilweights-without-special-char/weights').__next__().path","metadata":{"execution":{"iopub.status.busy":"2022-10-06T15:51:29.70606Z","iopub.execute_input":"2022-10-06T15:51:29.706421Z","iopub.status.idle":"2022-10-06T15:51:29.721475Z","shell.execute_reply.started":"2022-10-06T15:51:29.706389Z","shell.execute_reply":"2022-10-06T15:51:29.720542Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Load the file\ntext = open(DATA_PATH, \"r\", encoding=\"utf-8\").read().lower()\nsentences = re.split('[.!?]', text)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T15:57:24.591503Z","iopub.execute_input":"2022-10-06T15:57:24.591868Z","iopub.status.idle":"2022-10-06T15:57:24.762584Z","shell.execute_reply.started":"2022-10-06T15:57:24.591836Z","shell.execute_reply":"2022-10-06T15:57:24.761602Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lenghts = [len(sentence.split()) for sentence in sentences]\nprint(np.percentile(lenghts, 75))\n\nplt.figure(figsize=(10,10))\nplt.plot(lenghts)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = text.split()\nwords_unique = Counter(words).most_common()\ndictionary = {}\nfor word in words_unique:\n    dictionary[word[0]] = word[1]\ndict_values = list(dictionary.values())\n\nplt.figure(figsize=(10,10))\nplt.plot(dict_values)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformers\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", \n                                          bos_token='<|startoftext|>', \n                                          eos_token='<|endoftext|>', \n                                          pad_token='<|pad|>')\nmodel = AutoModelForCausalLM.from_pretrained(SAVED_MODEL_PATH, local_files_only=True)\n#model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\") #weights for fine tuning\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-10-06T15:57:27.950772Z","iopub.execute_input":"2022-10-06T15:57:27.95112Z","iopub.status.idle":"2022-10-06T15:57:35.87995Z","shell.execute_reply.started":"2022-10-06T15:57:27.951088Z","shell.execute_reply":"2022-10-06T15:57:35.878228Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Embedding(50259, 768)"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LENGTH = 85\n\nclass Harry_dataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_length):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attn_masks = []\n        #self.labels = []\n        for sentence in sentences:\n            encodings_dict = tokenizer('<|startoftext|>' + sentence + '<|endoftext|>', truncation=True,\n                                       max_length=max_length, padding=\"max_length\")\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]\n    \n\ndataset = Harry_dataset(sentences, tokenizer, max_length=MAX_LENGTH)\ntrain_dataset = dataset\n#train_size = int(0.9 * len(dataset))\n#train_dataset = Subset(dataset, list(range(0, train_size)))\n#val_dataset = Subset(dataset, list(range(train_size, len(dataset))))","metadata":{"execution":{"iopub.status.busy":"2022-10-06T15:57:35.882495Z","iopub.execute_input":"2022-10-06T15:57:35.883623Z","iopub.status.idle":"2022-10-06T15:57:54.085597Z","shell.execute_reply.started":"2022-10-06T15:57:35.883586Z","shell.execute_reply":"2022-10-06T15:57:54.084623Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir='./weights', num_train_epochs=10, logging_steps=1000,\n                                  logging_strategy='steps', save_strategy='epoch',\n                                  per_device_train_batch_size=32, learning_rate=1e-5,\n                                  warmup_steps=10, save_total_limit=1, weight_decay=0.05, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2022-10-06T19:41:59.282923Z","iopub.execute_input":"2022-10-06T19:41:59.283285Z","iopub.status.idle":"2022-10-06T19:41:59.29067Z","shell.execute_reply.started":"2022-10-06T19:41:59.283253Z","shell.execute_reply":"2022-10-06T19:41:59.289677Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"Trainer.train(resume_from_checkpoint=SAVED_MODEL_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trainer.train(resume_from_checkpoint=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train()","metadata":{"execution":{"iopub.status.busy":"2022-10-06T19:42:00.566497Z","iopub.execute_input":"2022-10-06T19:42:00.566821Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 85593\n  Num Epochs = 10\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 26750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='861' max='26750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  861/26750 03:48 < 1:54:40, 3.76 it/s, Epoch 0.32/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss after training - 0.17","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin = 'Mr . and Mrs . Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much . They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .'\n#generated = tokenizer.encode(begin, return_tensors='pt').cuda()\ngenerated = tokenizer.encode(begin, return_tensors='pt').cuda()\nattention_mask = torch.ones_like(generated)\nsample_outputs = model.generate(generated, do_sample=True, top_k=20, max_new_tokens=400, min_length=200, top_p=1, temperature=1.6, no_repeat_ngram_size=5, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)[0]\ntokenizer.decode(sample_outputs, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-06T18:03:27.320432Z","iopub.execute_input":"2022-10-06T18:03:27.320981Z","iopub.status.idle":"2022-10-06T18:03:28.744682Z","shell.execute_reply.started":"2022-10-06T18:03:27.32094Z","shell.execute_reply":"2022-10-06T18:03:28.743717Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'Mr. and Mrs. Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much. They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense. ive no alternative ive always always been asked to approaching sorts of first year vernon dursley shacklebolt ive been saying the ministry these days went to neighbor three of wizarding past midnight when steak dursleys twin cores of the burrow vernons keeper is quiddly dies sirius black obviously knows they got the knocking over dinner poured vernon shortly vernon give us vernon are getting heavily wrong vernon are going to butterbeer vernons mum and phlegmarge are getting old family number two hours vernon vernon ive been quiddles all summer vernon ickle marge vernon comes too vernon give me this must die havent got ze n mum and ear deafening the hammering me '"},"metadata":{}}]},{"cell_type":"code","source":"#compress folder to zip file\nimport shutil\nshutil.make_archive(\"GPT2_weights\", 'zip', \"./checkpoint-25000\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}