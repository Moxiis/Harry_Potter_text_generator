{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/moxxis/harry-potter-text-generator-transformers?scriptVersionId=107965045\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# IMPORTS\nimport re\nimport os\nimport torch\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom torch.utils.data import Dataset, Subset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, TrainingArguments, Trainer, TrainerCallback","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:58:56.317615Z","iopub.execute_input":"2022-10-13T08:58:56.318235Z","iopub.status.idle":"2022-10-13T08:58:56.327266Z","shell.execute_reply.started":"2022-10-13T08:58:56.318168Z","shell.execute_reply":"2022-10-13T08:58:56.32636Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#DIRECTORIES\nDATA_PATH = \"../input/harry-potter-lstm/Harry_Potter_all_char.txt\"\nSAVED_MODEL_PATH = os.scandir('/kaggle/input/harry-potter-text-generator-transformers/weights').__next__().path","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:58:57.034031Z","iopub.execute_input":"2022-10-13T08:58:57.034485Z","iopub.status.idle":"2022-10-13T08:58:57.04421Z","shell.execute_reply.started":"2022-10-13T08:58:57.034447Z","shell.execute_reply":"2022-10-13T08:58:57.043292Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Load the file\ntext = open(DATA_PATH, \"r\", encoding=\"utf-8\").read()\nsentences = re.split('\\|', text)\n\n#merge short sentences with larger next to them\nfor i in range(3):\n    print(len(sentences))\n    for idx, sentence in enumerate(sentences):\n        if len(sentence.split()) < 3 and idx-1>=0:\n            try:\n                if len(sentences[idx-1]) > len(sentences[idx+1]):\n                    sentences[idx+1] += sentence\n                    sentences.pop(idx)\n                else:\n                    sentences[idx-1] += sentence\n                    sentences.pop(idx)\n            except:\n                    sentences[idx-1] += sentence\n                    sentences.pop(idx)\n    print(len(sentences))                ","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:58:57.744524Z","iopub.execute_input":"2022-10-13T08:58:57.744961Z","iopub.status.idle":"2022-10-13T08:58:58.15688Z","shell.execute_reply.started":"2022-10-13T08:58:57.74492Z","shell.execute_reply":"2022-10-13T08:58:58.15594Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"79731\n74391\n74391\n74289\n74289\n74289\n","output_type":"stream"}]},{"cell_type":"code","source":"lenghts = [len(sentence.split()) for sentence in sentences]\nprint(np.percentile(lenghts, 75))\n\nplt.figure(figsize=(10,10))\nplt.plot(lenghts)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = text.split()\nwords_unique = Counter(words).most_common()\ndictionary = {}\nfor word in words_unique:\n    dictionary[word[0]] = word[1]\ndict_values = list(dictionary.values())\n\nplt.figure(figsize=(10,10))\nplt.plot(dict_values)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformers\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", \n                                          bos_token='<|startoftext|>', \n                                          eos_token='<|endoftext|>', \n                                          pad_token='<|pad|>')\nmodel = AutoModelForCausalLM.from_pretrained(SAVED_MODEL_PATH, local_files_only=True)\n#model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\") #weights for fine tuning\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:59:00.079694Z","iopub.execute_input":"2022-10-13T08:59:00.080131Z","iopub.status.idle":"2022-10-13T08:59:12.565646Z","shell.execute_reply.started":"2022-10-13T08:59:00.080092Z","shell.execute_reply":"2022-10-13T08:59:12.564758Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e1f3c49c824af89729b68fc4385bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d590b7677d184efba6953879a43b4c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbe9f58817de4a899bef362d3cc9b65c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8776835d929d4746a078a227e1733ec4"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Embedding(50259, 768)"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LENGTH = 100\n\nclass Harry_dataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_length):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attn_masks = []\n        for sentence in sentences:\n            encodings_dict = tokenizer('<|startoftext|>' + sentence + '<|endoftext|>', truncation=True,\n                                       max_length=max_length, padding=\"max_length\")\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]\n    \n\ndataset = Harry_dataset(sentences, tokenizer, max_length=MAX_LENGTH)\ntrain_dataset = dataset\n#train_size = int(0.9 * len(dataset))\n#train_dataset = Subset(dataset, list(range(0, train_size)))\n#val_dataset = Subset(dataset, list(range(train_size, len(dataset))))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:59:12.567546Z","iopub.execute_input":"2022-10-13T08:59:12.567912Z","iopub.status.idle":"2022-10-13T08:59:29.00648Z","shell.execute_reply.started":"2022-10-13T08:59:12.567874Z","shell.execute_reply":"2022-10-13T08:59:29.00549Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir='./weights', num_train_epochs=30, logging_steps=1000,\n                                  logging_strategy='steps', save_strategy='epoch',\n                                  per_device_train_batch_size=32,\n                                  warmup_steps=10, save_total_limit=1, weight_decay=0.05, report_to='none')\n\n#Callback: after epoch generate new text examples\nclass DefaultFlowCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n        now = datetime.now()\n        references = sentences[50:60]\n        examples = []\n        for reference in references:\n            generated = tokenizer.encode(reference, return_tensors='pt').cuda()\n            attention_mask = torch.ones_like(generated)\n            sample_outputs = model.generate(generated, do_sample=True, top_k=20, max_new_tokens=400, min_length=100, top_p=0.95, temperature=1.6, no_repeat_ngram_size=5, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)[0]\n            examples.append(tokenizer.decode(sample_outputs, skip_special_tokens=True))\n        with open(f\"example - {now.strftime('%d-%m||%H:%M')}.txt\", 'w+') as file:\n            for idx, ref in enumerate(references):\n                file.write(f'{ref}\\n\\n')\n                file.write(f'{examples[idx]}\\n\\n\\n\\n')  ","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:59:29.008491Z","iopub.execute_input":"2022-10-13T08:59:29.008955Z","iopub.status.idle":"2022-10-13T08:59:29.019953Z","shell.execute_reply.started":"2022-10-13T08:59:29.008917Z","shell.execute_reply":"2022-10-13T08:59:29.018828Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"Trainer(model=model,  args=training_args, callbacks=[DefaultFlowCallback], train_dataset=train_dataset, \n        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train(resume_from_checkpoit=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T20:43:06.743304Z","iopub.execute_input":"2022-10-12T20:43:06.746808Z","iopub.status.idle":"2022-10-12T20:43:06.875245Z","shell.execute_reply.started":"2022-10-12T20:43:06.74671Z","shell.execute_reply":"2022-10-12T20:43:06.87248Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_5456/670495571.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Trainer(model=model,  args=training_args, callbacks=[DefaultFlowCallback], train_dataset=train_dataset, \n\u001b[0m\u001b[1;32m      2\u001b[0m         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n\u001b[1;32m      3\u001b[0m                                                               \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                               'labels': torch.stack([f[0] for f in data])}).train(resume_from_checkpoit=True)\n","\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"],"ename":"NameError","evalue":"name 'Trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"Trainer(model=model,  args=training_args, callbacks=[DefaultFlowCallback], train_dataset=train_dataset, \n        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train(SAVED_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:59:29.022406Z","iopub.execute_input":"2022-10-13T08:59:29.023059Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Loading model from /kaggle/input/harry-potter-text-generator-transformers/weights/checkpoint-34830.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 74289\n  Num Epochs = 30\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 69660\n  Continuing training from checkpoint, will skip to saved global_step\n  Continuing training from epoch 15\n  Continuing training from global step 34830\n  Will skip the first 15 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e9ef3080094483895a473e49ff137a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='65592' max='69660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [65592/69660 2:34:27 < 20:25, 3.32 it/s, Epoch 28.25/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>35000</td>\n      <td>0.343200</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.340900</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.341200</td>\n    </tr>\n    <tr>\n      <td>38000</td>\n      <td>0.330500</td>\n    </tr>\n    <tr>\n      <td>39000</td>\n      <td>0.329200</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>0.321100</td>\n    </tr>\n    <tr>\n      <td>41000</td>\n      <td>0.318600</td>\n    </tr>\n    <tr>\n      <td>42000</td>\n      <td>0.313000</td>\n    </tr>\n    <tr>\n      <td>43000</td>\n      <td>0.307200</td>\n    </tr>\n    <tr>\n      <td>44000</td>\n      <td>0.309200</td>\n    </tr>\n    <tr>\n      <td>45000</td>\n      <td>0.300700</td>\n    </tr>\n    <tr>\n      <td>46000</td>\n      <td>0.299600</td>\n    </tr>\n    <tr>\n      <td>47000</td>\n      <td>0.296500</td>\n    </tr>\n    <tr>\n      <td>48000</td>\n      <td>0.294000</td>\n    </tr>\n    <tr>\n      <td>49000</td>\n      <td>0.291800</td>\n    </tr>\n    <tr>\n      <td>50000</td>\n      <td>0.287200</td>\n    </tr>\n    <tr>\n      <td>51000</td>\n      <td>0.288900</td>\n    </tr>\n    <tr>\n      <td>52000</td>\n      <td>0.282300</td>\n    </tr>\n    <tr>\n      <td>53000</td>\n      <td>0.284400</td>\n    </tr>\n    <tr>\n      <td>54000</td>\n      <td>0.280500</td>\n    </tr>\n    <tr>\n      <td>55000</td>\n      <td>0.279500</td>\n    </tr>\n    <tr>\n      <td>56000</td>\n      <td>0.277000</td>\n    </tr>\n    <tr>\n      <td>57000</td>\n      <td>0.273900</td>\n    </tr>\n    <tr>\n      <td>58000</td>\n      <td>0.276100</td>\n    </tr>\n    <tr>\n      <td>59000</td>\n      <td>0.272400</td>\n    </tr>\n    <tr>\n      <td>60000</td>\n      <td>0.271700</td>\n    </tr>\n    <tr>\n      <td>61000</td>\n      <td>0.271000</td>\n    </tr>\n    <tr>\n      <td>62000</td>\n      <td>0.268800</td>\n    </tr>\n    <tr>\n      <td>63000</td>\n      <td>0.270400</td>\n    </tr>\n    <tr>\n      <td>64000</td>\n      <td>0.266700</td>\n    </tr>\n    <tr>\n      <td>65000</td>\n      <td>0.268200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./weights/checkpoint-37152\nConfiguration saved in ./weights/checkpoint-37152/config.json\nModel weights saved in ./weights/checkpoint-37152/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-34830] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-39474\nConfiguration saved in ./weights/checkpoint-39474/config.json\nModel weights saved in ./weights/checkpoint-39474/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-37152] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-41796\nConfiguration saved in ./weights/checkpoint-41796/config.json\nModel weights saved in ./weights/checkpoint-41796/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-39474] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-44118\nConfiguration saved in ./weights/checkpoint-44118/config.json\nModel weights saved in ./weights/checkpoint-44118/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-41796] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-46440\nConfiguration saved in ./weights/checkpoint-46440/config.json\nModel weights saved in ./weights/checkpoint-46440/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-44118] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-48762\nConfiguration saved in ./weights/checkpoint-48762/config.json\nModel weights saved in ./weights/checkpoint-48762/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-46440] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-51084\nConfiguration saved in ./weights/checkpoint-51084/config.json\nModel weights saved in ./weights/checkpoint-51084/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-48762] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-53406\nConfiguration saved in ./weights/checkpoint-53406/config.json\nModel weights saved in ./weights/checkpoint-53406/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-51084] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-55728\nConfiguration saved in ./weights/checkpoint-55728/config.json\nModel weights saved in ./weights/checkpoint-55728/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-53406] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-58050\nConfiguration saved in ./weights/checkpoint-58050/config.json\nModel weights saved in ./weights/checkpoint-58050/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-55728] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-60372\nConfiguration saved in ./weights/checkpoint-60372/config.json\nModel weights saved in ./weights/checkpoint-60372/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-58050] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-62694\nConfiguration saved in ./weights/checkpoint-62694/config.json\nModel weights saved in ./weights/checkpoint-62694/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-60372] due to args.save_total_limit\nSaving model checkpoint to ./weights/checkpoint-65016\nConfiguration saved in ./weights/checkpoint-65016/config.json\nModel weights saved in ./weights/checkpoint-65016/pytorch_model.bin\nDeleting older checkpoint [weights/checkpoint-62694] due to args.save_total_limit\n","output_type":"stream"}]},{"cell_type":"code","source":"Trainer(model=model,  args=training_args, callbacks=[DefaultFlowCallback], train_dataset=train_dataset, \n        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                              'attention_mask': torch.stack([f[1] for f in data]),\n                                                              'labels': torch.stack([f[0] for f in data])}).train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss after training - 0.13","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin = 'Mr . and Mrs . Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much . They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .'\n#generated = tokenizer.encode(begin, return_tensors='pt').cuda()\ngenerated = tokenizer.encode(begin, return_tensors='pt').cuda()\nattention_mask = torch.ones_like(generated)\nsample_outputs = model.generate(generated, do_sample=True, top_k=20, max_new_tokens=400, min_length=200, top_p=1, temperature=1.6, no_repeat_ngram_size=5, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)[0]\ntokenizer.decode(sample_outputs, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compress folder to zip file\nimport shutil\nshutil.make_archive(\"GPT2_weights\", 'zip', \"./checkpoint-25000\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}